{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avlis-MMO/Deep_Learning/blob/main/MIT_DL/MITLab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7oxgh5HHJfE"
      },
      "source": [
        "\n",
        "# **Copyright Information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjMsDQVwHYGt"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Copyright 2023 MIT Introduction to Deep Learning. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of MIT Introduction\n",
        "# to Deep Learning must reference:\n",
        "#\n",
        "# Â© MIT Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ykWAj60fLz"
      },
      "source": [
        "# **Lab 1**\n",
        "# **Part 2: Music Generation with RNNs**\n",
        "# **2.1 Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLBvznA4CWBH"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download and import the MIT Introduction to Deep Learning package\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import functools\n",
        "from IPython import display as ipythondisplay\n",
        "from tqdm import tqdm\n",
        "!apt-get install abcmidi timidity > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmuB2GnQ0XvD"
      },
      "source": [
        "# **2.2 Dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKr4LeQBF4TG"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "songs = mdl.lab1.load_training_data()\n",
        "\n",
        "# Print one of the songs to inspect it in greater detail!\n",
        "example_song = songs[0]\n",
        "print(\"\\nExample song: \")\n",
        "print(example_song)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8Xv3sKjHp3s"
      },
      "outputs": [],
      "source": [
        "# Convert the ABC notation to audio file and listen to it\n",
        "mdl.lab1.play_song(example_song)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVc_QzdTH4Kz"
      },
      "outputs": [],
      "source": [
        "# Join our list of song strings into a single string containing all songs\n",
        "songs_joined = \"\\n\\n\".join(songs)\n",
        "print(songs_joined)\n",
        "# Find all unique characters in the joined string\n",
        "vocab = sorted(set(songs_joined))\n",
        "#print(vocab)\n",
        "print(\"There are\", len(vocab), \"unique characters in the dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwCZy48d0U7A"
      },
      "source": [
        "# **2.3 Process the dataset for the learning task**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKoghvM5I831"
      },
      "outputs": [],
      "source": [
        "### Define numerical representation of text ###\n",
        "\n",
        "# Create a mapping from character to unique index.\n",
        "# For example, to get the index of the character \"d\",\n",
        "#   we can evaluate `char2idx[\"d\"]`.\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "# Create a mapping from indices to characters. This is\n",
        "#   the inverse of char2idx and allows us to convert back\n",
        "#   from unique index to the character in our vocabulary.\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XOMXVtnJRxw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def vectorize_string(string):\n",
        "  vect_songs = []\n",
        "  for i in songs_joined:\n",
        "    vect_songs.append(char2idx[i])\n",
        "\n",
        "  return np.array(vect_songs)\n",
        "\n",
        "vectorized_songs = vectorize_string(songs_joined)\n",
        "print(vectorized_songs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSWUqhuDKy-v"
      },
      "outputs": [],
      "source": [
        "print ('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:10]), vectorized_songs[:10]))\n",
        "# check that vectorized_songs is a numpy array\n",
        "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a numpy array\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XPfxzXpLqer"
      },
      "outputs": [],
      "source": [
        "### Batch definition to create training examples ###\n",
        "\n",
        "def get_batch(vectorized_songs, seq_length, batch_size):\n",
        "  # the length of the vectorized songs string\n",
        "  n = vectorized_songs.shape[0] - 1\n",
        "\n",
        "  # randomly choose the starting indices for the examples in the training batch\n",
        "  idx = np.random.choice(n-seq_length, batch_size)\n",
        "\n",
        "  input_batch = []\n",
        "  output_batch = []\n",
        "  for i in idx:\n",
        "    input_batch.append(vectorized_songs[i:i+seq_length])\n",
        "\n",
        "    output_batch.append(vectorized_songs[i+1:i+1+seq_length])\n",
        "  print(input_batch)\n",
        "  # x_batch, y_batch provide the true inputs and targets for network training\n",
        "  x_batch = np.reshape(input_batch, (batch_size, seq_length))\n",
        "  y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
        "  return x_batch, y_batch\n",
        "\n",
        "\n",
        "# Perform some simple tests to make sure your batch function is working properly!\n",
        "test_args = (vectorized_songs, 10, 2)\n",
        "if not mdl.lab1.test_batch_func_types(get_batch, test_args) or \\\n",
        "   not mdl.lab1.test_batch_func_shapes(get_batch, test_args) or \\\n",
        "   not mdl.lab1.test_batch_func_next_step(get_batch, test_args):\n",
        "   print(\"======\\n[FAIL] could not pass tests\")\n",
        "else:\n",
        "   print(\"======\\n[PASS] passed all tests!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woX_PnOGQKX4"
      },
      "outputs": [],
      "source": [
        "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=2)\n",
        "print(y_batch.shape)\n",
        "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\n",
        "    print(\"Step {:3d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yDmto3q0KJR"
      },
      "source": [
        "# **2.4 The Recurrent Neural Network (RNN) mode**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANeeuz20Q6VU"
      },
      "outputs": [],
      "source": [
        "def LSTM(rnn_units):\n",
        "  return tf.keras.layers.LSTM(\n",
        "    rnn_units,\n",
        "    return_sequences=True,\n",
        "    recurrent_initializer='glorot_uniform',\n",
        "    recurrent_activation='sigmoid',\n",
        "    stateful=True,\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHqDQtQBQ_D2"
      },
      "outputs": [],
      "source": [
        "### Defining the RNN Model ###\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    # Layer 1: Embedding layer to transform indices into dense vectors\n",
        "    #   of a fixed embedding size\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "\n",
        "    # Layer 2: LSTM with `rnn_units` number of units.\n",
        "    LSTM(rnn_units),\n",
        "\n",
        "    # Layer 3: Dense (fully-connected) layer that transforms the LSTM output\n",
        "    #   into the vocabulary size.\n",
        "    tf.keras.layers.Dense(vocab_size, activation='relu')\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "# Build a simple model with default hyperparameters. You will get the\n",
        "#   chance to change these later.\n",
        "model = build_model(len(vocab), embedding_dim=256, rnn_units=1024, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LnVPa2ERnoz"
      },
      "outputs": [],
      "source": [
        "model.summary()\n",
        "\n",
        "x, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\n",
        "pred = model(x)\n",
        "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
        "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i04j32WQn0WW"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(pred[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX1Xxvimn3pu"
      },
      "outputs": [],
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcZX_4sFo1SF"
      },
      "source": [
        "# **2.5 Training the model: loss and training operations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8j71yXoo3Vt"
      },
      "outputs": [],
      "source": [
        "### Defining the loss function ###\n",
        "\n",
        "def compute_loss(labels, logits):\n",
        "  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "  return loss\n",
        "\n",
        "example_batch_loss = compute_loss(y, pred)\n",
        "\n",
        "print(\"Prediction shape: \", pred.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICU8Xy6FqbpB"
      },
      "outputs": [],
      "source": [
        "\n",
        "### Hyperparameter setting and optimization ###\n",
        "\n",
        "# Optimization parameters:\n",
        "num_training_iterations = 2000  # Increase this to train longer\n",
        "batch_size = 32  # Experiment between 1 and 64\n",
        "seq_length = 100  # Experiment between 50 and 500\n",
        "learning_rate = 5e-3  # Experiment between 1e-5 and 1e-1\n",
        "\n",
        "# Model parameters:\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024  # Experiment between 1 and 2048\n",
        "\n",
        "# Checkpoint location:\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkjHehG0qviV"
      },
      "outputs": [],
      "source": [
        "### Define optimizer and training operation ###\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "  # Use tf.GradientTape()\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    y_hat = model(x)\n",
        "\n",
        "    loss = compute_loss(y, y_hat)\n",
        "\n",
        "  # Now, compute the gradients\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # Apply the gradients to the optimizer so it can update the model accordingly\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "##################\n",
        "# Begin training!#\n",
        "##################\n",
        "\n",
        "history = []\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "\n",
        "for iter in tqdm(range(num_training_iterations)):\n",
        "\n",
        "  # Grab a batch and propagate it through the network\n",
        "  x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\n",
        "  loss = train_step(x_batch, y_batch)\n",
        "\n",
        "  # Update the progress bar\n",
        "  history.append(loss.numpy().mean())\n",
        "  plotter.plot(history)\n",
        "\n",
        "  # Update the model with the changed weights!\n",
        "  if iter % 100 == 0:\n",
        "    model.save_weights(checkpoint_prefix)\n",
        "\n",
        "# Save the trained model and the weights\n",
        "model.save_weights(checkpoint_prefix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ-r01CYs7mu"
      },
      "source": [
        "# **2.6 Generate music using the RNN model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk4OiHN6sCfz"
      },
      "outputs": [],
      "source": [
        "# Trasnform to batch size 1\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "# Restore the model weights for the last checkpoint after training\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JsGZdTXs9wV"
      },
      "outputs": [],
      "source": [
        "### Prediction of a generated song ###\n",
        "\n",
        "def generate_text(model, start_string, generation_length=1000):\n",
        "  # Evaluation step (generating ABC text using the learned RNN model)\n",
        "\n",
        "  input_eval = []\n",
        "  for ch in start_string:\n",
        "    input_eval.append(char2idx[ch])\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  tqdm._instances.clear()\n",
        "\n",
        "  for i in tqdm(range(generation_length)):\n",
        "\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      # Remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # Pass the prediction along with the previous hidden state\n",
        "      #   as the next inputs to the model\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      # Hint: consider what format the prediction is in vs. the output\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nh99fiEvMv4"
      },
      "outputs": [],
      "source": [
        "generated_text = generate_text(model, start_string=\"X\", generation_length=1000)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NopMSM8Uvzkd"
      },
      "outputs": [],
      "source": [
        "### Play back generated songs ###\n",
        "\n",
        "generated_songs = mdl.lab1.extract_song_snippet(generated_text)\n",
        "print(generated_text)\n",
        "for i, song in enumerate(generated_songs):\n",
        "  # Synthesize the waveform from a song\n",
        "  waveform = mdl.lab1.play_song(song)\n",
        "\n",
        "  # If its a valid song (correct syntax), lets play it!\n",
        "  if waveform:\n",
        "    print(\"Generated song\", i)\n",
        "    ipythondisplay.display(waveform)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCAqvAgRzdmwelaB2qTxGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}