{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1r1Uo_3ayFtc5qFH5-aIJ2_G7ZD6zJ1Sh",
      "authorship_tag": "ABX9TyPFO26Zk7Boj7J0ZccghXID",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avlis-MMO/Deep_Learning/blob/main/MIT_DL/MITLab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Copyright Information**"
      ],
      "metadata": {
        "id": "5Kq9GcdfwkLr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlIK92p0wjee"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 MIT Introduction to Deep Learning. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of MIT Introduction\n",
        "# to Deep Learning must reference:\n",
        "#\n",
        "# Â© MIT Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 3**\n",
        "# **1.1 Dependencies**"
      ],
      "metadata": {
        "id": "GQjVRjX-wncB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Tensorflow 2.0\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Download and import the MIT 6.S191 package\n",
        "!printf \"Installing MIT deep learning package... \"\n",
        "!pip install --upgrade git+https://github.com/aamini/introtodeeplearning.git &> /dev/null\n",
        "!echo \"Done\"\n",
        "\n",
        "#Install some dependencies for visualizing the agents\n",
        "!apt-get install -y xvfb python-opengl x11-utils &> /dev/null\n",
        "!pip install gym pyvirtualdisplay scikit-video ffio pyrender &> /dev/null\n",
        "!pip install tensorflow_probability==0.12.0 &> /dev/null\n",
        "!pip install xvfbwrapper\n",
        "!apt install chromium-browser xvfb\n",
        "import os\n",
        "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib, cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import base64, io, os, time, gym\n",
        "import IPython, functools\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "\n",
        "import mitdeeplearning as mdl"
      ],
      "metadata": {
        "id": "Tw3R_b8dxDBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Cartpole**\n",
        "# **3.1 Define the Cartpole environment and agent**"
      ],
      "metadata": {
        "id": "nwg1B1rMxPT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Instantiate the Cartpole environment ###\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env.seed(1)\n",
        "\n",
        "n_observations = env.observation_space\n",
        "print(\"Environment has observation space =\", n_observations)\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "print(\"Number of possible actions that the agent can choose from =\", n_actions)"
      ],
      "metadata": {
        "id": "a33HtvwZxgrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Define the Cartpole agent ###\n",
        "\n",
        "# Defines a feed-forward neural network\n",
        "def create_cartpole_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # First Dense layer\n",
        "        tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=n_actions)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "cartpole_model = create_cartpole_model()"
      ],
      "metadata": {
        "id": "_i_e-fwQ1f2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Define the agent's action function ###\n",
        "\n",
        "# Function that takes observations as input, executes a forward pass through model,\n",
        "#   and outputs a sampled action.\n",
        "# Arguments:\n",
        "#   model: the network that defines our agent\n",
        "#   observation: observation(s) which is/are fed as input to the model\n",
        "#   single: flag as to whether we are handling a single observation or batch of\n",
        "#     observations, provided as an np.array\n",
        "# Returns:\n",
        "#   action: choice of agent action\n",
        "def choose_action(model, observation, single=True):\n",
        "    # add batch dimension to the observation if only a single example was provided\n",
        "    observation = np.expand_dims(observation, axis=0) if single else observation\n",
        "\n",
        "    logits = model.predict(observation)\n",
        "\n",
        "    action = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "    action = action.numpy().flatten()\n",
        "\n",
        "    return action[0] if single else action"
      ],
      "metadata": {
        "id": "MMGvLmHR2JMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.2 Define the agent's memory**"
      ],
      "metadata": {
        "id": "TzcEZeLu4Ybt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Agent Memory ###\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.clear()\n",
        "\n",
        "  # Resets/restarts the memory buffer\n",
        "    def clear(self):\n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "  # Add observations, actions, rewards to memory\n",
        "    def add_to_memory(self, new_observation, new_action, new_reward):\n",
        "        self.observations.append(new_observation)\n",
        "        self.actions.append(new_action)\n",
        "        self.rewards.append(new_reward)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.actions)\n",
        "\n",
        "# Instantiate a single Memory buffer\n",
        "memory = Memory()"
      ],
      "metadata": {
        "id": "mMIg7J_-4X-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.3 Reward function**"
      ],
      "metadata": {
        "id": "lMyot_so5Aii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Reward function ###\n",
        "\n",
        "# Helper function that normalizes an np.array x\n",
        "def normalize(x):\n",
        "    x -= np.mean(x)\n",
        "    x /= np.std(x)\n",
        "    return x.astype(np.float32)\n",
        "\n",
        "# Compute normalized, discounted, cumulative rewards (i.e., return)\n",
        "# Arguments:\n",
        "#   rewards: reward at timesteps in episode\n",
        "#   gamma: discounting factor\n",
        "# Returns:\n",
        "#   normalized discounted reward\n",
        "def discount_rewards(rewards, gamma=0.95):\n",
        "    discounted_rewards = np.zeros_like(rewards)\n",
        "    R = 0\n",
        "    for t in reversed(range(0, len(rewards))):\n",
        "        # update the total discounted reward\n",
        "        R = R * gamma + rewards[t]\n",
        "        discounted_rewards[t] = R\n",
        "\n",
        "    return normalize(discounted_rewards)"
      ],
      "metadata": {
        "id": "AVKuu8_u5ABK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.4 Learning algorithm**"
      ],
      "metadata": {
        "id": "_uUuG6L-6H_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Loss function ###\n",
        "\n",
        "# Arguments:\n",
        "#   logits: network's predictions for actions to take\n",
        "#   actions: the actions the agent took in an episode\n",
        "#   rewards: the rewards the agent received in an episode\n",
        "# Returns:\n",
        "#   loss\n",
        "def compute_loss(logits, actions, rewards):\n",
        "    neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions)\n",
        "\n",
        "    loss = tf.reduce_mean(neg_logprob * rewards)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "_lTqlIzz6Knd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Training step (forward and backpropagation) ###\n",
        "\n",
        "def train_step(model, loss_function, optimizer, observations, actions, discounted_rewards, custom_fwd_fn=None):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward propagate through the agent network\n",
        "        if custom_fwd_fn is not None:\n",
        "            prediction = custom_fwd_fn(observations)\n",
        "        else:\n",
        "            prediction = model(observations)\n",
        "\n",
        "        loss = loss_function(prediction, actions, discounted_rewards)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    grads, _ = tf.clip_by_global_norm(grads, 1.5)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
      ],
      "metadata": {
        "id": "si0BNqCo7A51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.5 Run cartpole!**"
      ],
      "metadata": {
        "id": "3evi7IGu8eYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Training parameters ##\n",
        "## Re-run this cell to restart training from scratch ##\n",
        "\n",
        "learning_rate = 1e-3\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# instantiate cartpole agent\n",
        "cartpole_model = create_cartpole_model()\n",
        "\n",
        "# to track our progress\n",
        "smoothed_reward = mdl.util.LossHistory(smoothing_factor=0.95)\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Rewards')"
      ],
      "metadata": {
        "id": "ZrOj6kEb8k1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cartpole training! ##\n",
        "## Note: stoping and restarting this cell will pick up training where you\n",
        "#        left off. To restart training you need to rerun the cell above as\n",
        "#        well (to re-initialize the model and optimizer)\n",
        "\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "for i_episode in range(350):\n",
        "\n",
        "    plotter.plot(smoothed_reward.get())\n",
        "    # Restart the environment\n",
        "    observation = env.reset()\n",
        "    memory.clear()\n",
        "\n",
        "    while True:\n",
        "        # using our observation, choose an action and take it in the environment\n",
        "        action = choose_action(cartpole_model, observation)\n",
        "        next_observation, reward, done, info = env.step(action)\n",
        "        # add to memory\n",
        "        memory.add_to_memory(observation, action, reward)\n",
        "\n",
        "        # is the episode over? did you crash or do so well that you're done?\n",
        "        if done:\n",
        "            # determine total reward and keep a record of this\n",
        "            total_reward = sum(memory.rewards)\n",
        "            smoothed_reward.append(total_reward)\n",
        "\n",
        "            # initiate training - remember we don't know anything about how the\n",
        "            #   agent is doing until it has crashed!\n",
        "            g = train_step(cartpole_model, compute_loss, optimizer,\n",
        "                       observations=np.vstack(memory.observations),\n",
        "                       actions=np.array(memory.actions),\n",
        "                       discounted_rewards = discount_rewards(memory.rewards))\n",
        "\n",
        "            # reset the memory\n",
        "            memory.clear()\n",
        "            break\n",
        "        # update our observatons\n",
        "        observation = next_observation"
      ],
      "metadata": {
        "id": "6NAKlrJT9Tr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_JmGSyr8GA-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matplotlib.use('Agg')\n",
        "saved_cartpole = mdl.lab3_old.save_video_of_model(cartpole_model, \"CartPole-v1\")\n",
        "mdl.lab3_old.play_video(saved_cartpole)"
      ],
      "metadata": {
        "id": "oYa3fd_V948P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Training Autonomous Driving Policies in VISTA**"
      ],
      "metadata": {
        "id": "gJ92k9rBNzbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/vista-simulator/vista-6s191.git\n",
        "!pip install descartes"
      ],
      "metadata": {
        "id": "BSdgw-jwN57t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vista\n",
        "from vista.utils import logging\n",
        "logging.setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "qVkwCjowO5Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Access documentation for VISTA\n",
        "### Run ?vista.<[name of module or function]>\n",
        "?vista.Display"
      ],
      "metadata": {
        "id": "-lkc3nXPO821"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.6 Create an environment in VISTA**"
      ],
      "metadata": {
        "id": "9gkAUyTuO_RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract the data for vista (auto-skip if already downloaded)\n",
        "!wget -nc -q --show-progress https://www.dropbox.com/s/62pao4mipyzk3xu/vista_traces.zip\n",
        "print(\"Unzipping data...\")\n",
        "!unzip -o -q vista_traces.zip\n",
        "print(\"Done downloading and unzipping data!\")\n",
        "\n",
        "trace_root = \"./vista_traces\"\n",
        "trace_path = [\n",
        "    \"20210726-154641_lexus_devens_center\",\n",
        "    \"20210726-155941_lexus_devens_center_reverse\",\n",
        "    \"20210726-184624_lexus_devens_center\",\n",
        "    \"20210726-184956_lexus_devens_center_reverse\",\n",
        "]\n",
        "trace_path = [os.path.join(trace_root, p) for p in trace_path]\n",
        "\n",
        "# Create a virtual world with VISTA, the world is defined by a series of data traces\n",
        "world = vista.World(trace_path, trace_config={'road_width': 4})\n",
        "\n",
        "# Create a car in our virtual world. The car will be able to step and take different\n",
        "#   control actions. As the car moves, its sensors will simulate any changes it environment\n",
        "car = world.spawn_agent(\n",
        "    config={\n",
        "        'length': 5.,\n",
        "        'width': 2.,\n",
        "        'wheel_base': 2.78,\n",
        "        'steering_ratio': 14.7,\n",
        "        'lookahead_road': True\n",
        "    })\n",
        "\n",
        "# Create a camera on the car for synthesizing the sensor data that we can use to train with!\n",
        "camera = car.spawn_camera(config={'size': (200, 320)})\n",
        "\n",
        "# Define a rendering display so we can visualize the simulated car camera stream and also\n",
        "#   get see its physical location with respect to the road in its environment.\n",
        "display = vista.Display(world, display_config={\"gui_scale\": 2, \"vis_full_frame\": False})\n",
        "\n",
        "# Define a simple helper function that allows us to reset VISTA and the rendering display\n",
        "def vista_reset():\n",
        "    world.reset()\n",
        "    display.reset()\n",
        "vista_reset()"
      ],
      "metadata": {
        "id": "pPU587lzPA-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.7 Our virtual agent: the car**"
      ],
      "metadata": {
        "id": "uL_67mTLQWwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we define a step function, to allow our virtual agent to step\n",
        "# with a given control command through the environment\n",
        "# agent can act with a desired curvature (turning radius, like steering angle)\n",
        "# and desired speed. if either is not provided then this step function will\n",
        "# use whatever the human executed at that time in the real data.\n",
        "\n",
        "def vista_step(curvature=None, speed=None):\n",
        "    # Arguments:\n",
        "    #   curvature: curvature to step with\n",
        "    #   speed: speed to step with\n",
        "    if curvature is None:\n",
        "        curvature = car.trace.f_curvature(car.timestamp)\n",
        "    if speed is None:\n",
        "        speed = car.trace.f_speed(car.timestamp)\n",
        "\n",
        "    car.step_dynamics(action=np.array([curvature, speed]), dt=1/15.)\n",
        "    car.step_sensors()"
      ],
      "metadata": {
        "id": "N95E3_ShQVja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os, subprocess, cv2\n",
        "\n",
        "# Create a simple helper class that will assist us in storing videos of the render\n",
        "class VideoStream():\n",
        "    def __init__(self):\n",
        "        self.tmp = \"./tmp\"\n",
        "        if os.path.exists(self.tmp) and os.path.isdir(self.tmp):\n",
        "            shutil.rmtree(self.tmp)\n",
        "        os.mkdir(self.tmp)\n",
        "    def write(self, image, index):\n",
        "        cv2.imwrite(os.path.join(self.tmp, f\"{index:04}.png\"), image)\n",
        "    def save(self, fname):\n",
        "        cmd = f\"/usr/bin/ffmpeg -f image2 -i {self.tmp}/%04d.png -crf 0 -y {fname}\"\n",
        "        subprocess.call(cmd, shell=True)\n"
      ],
      "metadata": {
        "id": "MEpKRaBKSB7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Render and inspect a human trace ##\n",
        "\n",
        "vista_reset()\n",
        "stream = VideoStream()\n",
        "\n",
        "for i in tqdm(range(100)):\n",
        "    vista_step()\n",
        "\n",
        "    # Render and save the display\n",
        "    vis_img= display.render()\n",
        "    stream.write(vis_img[:, :, ::-1], index=i)\n",
        "    if car.done:\n",
        "        break\n",
        "\n",
        "print(\"Saving trajectory of human following...\")\n",
        "stream.save(\"human_follow.mp4\")\n",
        "mdl.lab3.play_video(\"human_follow.mp4\")"
      ],
      "metadata": {
        "id": "j8nW0joXSGBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define terminal states and crashing conditions ##\n",
        "\n",
        "def check_out_of_lane(car):\n",
        "    distance_from_center = np.abs(car.relative_state.x)\n",
        "    road_width = car.trace.road_width\n",
        "    half_road_width = road_width / 2\n",
        "    return distance_from_center > half_road_width\n",
        "\n",
        "def check_exceed_max_rot(car):\n",
        "    maximal_rotation = np.pi / 10.\n",
        "    current_rotation = np.abs(car.relative_state.yaw)\n",
        "    return current_rotation > maximal_rotation\n",
        "\n",
        "def check_crash(car):\n",
        "    return check_out_of_lane(car) or check_exceed_max_rot(car) or car.done"
      ],
      "metadata": {
        "id": "FQaYw_Y6UZPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Behavior with random control policy ##\n",
        "\n",
        "i = 0\n",
        "num_crashes = 5\n",
        "stream = VideoStream()\n",
        "\n",
        "for _ in range(num_crashes):\n",
        "    vista_reset()\n",
        "\n",
        "    while not check_crash(car):\n",
        "\n",
        "        # Sample a random curvature (between +/- 1/3), keep speed constant\n",
        "        curvature = np.random.uniform(-1/3, 1/3)\n",
        "\n",
        "        # Step the simulated car with the same action\n",
        "        vista_step(curvature=curvature)\n",
        "\n",
        "        # Render and save the display\n",
        "        vis_img = display.render()\n",
        "        stream.write(vis_img[:, :, ::-1], index=i)\n",
        "        i += 1\n",
        "\n",
        "    print(f\"Car crashed on step {i}\")\n",
        "    for _ in range(5):\n",
        "        stream.write(vis_img[:, :, ::-1], index=i)\n",
        "        i += 1\n",
        "\n",
        "print(\"Saving trajectory with random policy...\")\n",
        "stream.save(\"random_policy.mp4\")\n",
        "mdl.lab3_old.play_video(\"random_policy.mp4\")"
      ],
      "metadata": {
        "id": "EOLCtaH2UiSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Directly access the raw sensor observations of the simulated car\n",
        "vista_reset()\n",
        "full_obs = car.observations[camera.name]\n",
        "cv2_imshow(full_obs)\n",
        "## ROIs ##\n",
        "\n",
        "# Crop a smaller region of interest (ROI). This is necessary because:\n",
        "#   1. The full observation will have distortions on the edge as the car deviates from the human\n",
        "#   2. A smaller image of the environment will be easier for our model to learn from\n",
        "region_of_interest = camera.camera_param.get_roi()\n",
        "i1, j1, i2, j2 = region_of_interest\n",
        "cropped_obs = full_obs[i1:i2, j1:j2]\n",
        "cv2_imshow(cropped_obs)"
      ],
      "metadata": {
        "id": "ZVeqv3yoU0IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Data preprocessing functions ##\n",
        "\n",
        "def preprocess(full_obs):\n",
        "    # Extract ROI\n",
        "    i1, j1, i2, j2 = camera.camera_param.get_roi()\n",
        "    obs = full_obs[i1:i2, j1:j2]\n",
        "\n",
        "    # Rescale to [0, 1]\n",
        "    obs = obs / 255.\n",
        "    return obs\n",
        "\n",
        "def grab_and_preprocess_obs(car):\n",
        "    full_obs = car.observations[camera.name]\n",
        "    obs = preprocess(full_obs)\n",
        "    return obs"
      ],
      "metadata": {
        "id": "lvMpf_y2U5aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.9 Define the self-driving agent and learning algorithm**"
      ],
      "metadata": {
        "id": "s6yQ_1wrU9zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Define the self-driving agent ###\n",
        "# Note: we start with a template CNN architecture -- experiment away as you\n",
        "#   try to optimize your agent!\n",
        "\n",
        "# Functionally define layers for convenience\n",
        "# All convolutional layers will have ReLu activation\n",
        "act = tf.keras.activations.swish\n",
        "Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='valid', activation=act)\n",
        "Flatten = tf.keras.layers.Flatten\n",
        "Dense = tf.keras.layers.Dense\n",
        "\n",
        "# Defines a CNN for the self-driving agent\n",
        "def create_driving_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # Convolutional layers\n",
        "        # First, 32 5x5 filters and 2x2 stride\n",
        "        Conv2D(filters=32, kernel_size=5, strides=2),\n",
        "\n",
        "        # TODO: define convolutional layers with 48 5x5 filters and 2x2 stride\n",
        "        Conv2D(filters = 48, kernel_size=3, strides=2),\n",
        "\n",
        "        # TODO: define two convolutional layers with 64 3x3 filters and 2x2 stride\n",
        "        Conv2D(filters = 64, kernel_size=3, strides=2),\n",
        "\n",
        "        Flatten(),\n",
        "\n",
        "        # Fully connected layer and output\n",
        "        Dense(units=128, activation=act),\n",
        "\n",
        "        # TODO: define the output dimension of the last Dense layer.\n",
        "        #    Pay attention to the space the agent needs to act in.\n",
        "        #    Remember that this model is outputing a distribution of *continuous*\n",
        "        #    actions, which take a different shape than discrete actions.\n",
        "        #    How many outputs should there be to define a distribution?'''\n",
        "\n",
        "        Dense(units=2, activation=None)\n",
        "\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "driving_model = create_driving_model()"
      ],
      "metadata": {
        "id": "TDUB-t8HVsH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The self-driving learning algorithm ##\n",
        "\n",
        "# hyperparameters\n",
        "max_curvature = 1/8.\n",
        "max_std = 0.1\n",
        "\n",
        "def run_driving_model(image):\n",
        "    # Arguments:\n",
        "    #   image: an input image\n",
        "    # Returns:\n",
        "    #   pred_dist: predicted distribution of control actions\n",
        "    single_image_input = tf.rank(image) == 3  # missing 4th batch dimension\n",
        "    if single_image_input:\n",
        "        image = tf.expand_dims(image, axis=0)\n",
        "\n",
        "    distribution = driving_model(image)\n",
        "\n",
        "    mu, logsigma = tf.split(distribution, 2, axis=1)\n",
        "    mu = max_curvature * tf.tanh(mu) # conversion\n",
        "    sigma = max_std * tf.sigmoid(logsigma) + 0.005 # conversion\n",
        "\n",
        "    pred_dist = tfp.distributions.Normal(mu, sigma)\n",
        "\n",
        "    return pred_dist\n",
        "\n",
        "def compute_driving_loss(dist, actions, rewards):\n",
        "    # Arguments:\n",
        "    #   logits: network's predictions for actions to take\n",
        "    #   actions: the actions the agent took in an episode\n",
        "    #   rewards: the rewards the agent received in an episode\n",
        "    # Returns:\n",
        "    #   loss\n",
        "\n",
        "    neg_logprob = -1 * dist.log_prob(actions)\n",
        "\n",
        "    loss = tf.reduce_mean(neg_logprob * rewards)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "qMESxPiAXDM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.10 Train the self-driving agent**"
      ],
      "metadata": {
        "id": "NlrEGgS4YEdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Training parameters and initialization ##\n",
        "## Re-run this cell to restart training from scratch ##\n",
        "\n",
        "learning_rate = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# instantiate driving agent\n",
        "vista_reset()\n",
        "driving_model = create_driving_model()\n",
        "# NOTE: the variable driving_model will be used in run_driving_model execution\n",
        "\n",
        "# to track our progress\n",
        "smoothed_reward = mdl.util.LossHistory(smoothing_factor=0.9)\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Rewards')\n",
        "\n",
        "# instantiate Memory buffer\n",
        "memory = Memory()"
      ],
      "metadata": {
        "id": "JgFpHx0oYPuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Driving training! Main training block. ##\n",
        "## Note: stopping and restarting this cell will pick up training where you\n",
        "#        left off. To restart training you need to rerun the cell above as\n",
        "#        well (to re-initialize the model and optimizer)\n",
        "\n",
        "max_batch_size = 300\n",
        "max_reward = float('-inf') # keep track of the maximum reward acheived during training\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "for i_episode in range(500):\n",
        "\n",
        "    plotter.plot(smoothed_reward.get())\n",
        "    # Restart the environment\n",
        "    vista_reset()\n",
        "    memory.clear()\n",
        "    observation = grab_and_preprocess_obs(car)\n",
        "\n",
        "    while True:\n",
        "\n",
        "        curvature_dist = run_driving_model(observation)\n",
        "\n",
        "        curvature_action = curvature_dist.sample()[0,0]\n",
        "\n",
        "        # Step the simulated car with the same action\n",
        "        vista_step(curvature_action)\n",
        "        observation = grab_and_preprocess_obs(car)\n",
        "\n",
        "        reward = 1.0 if not check_crash(car) else 0.0\n",
        "\n",
        "        # add to memory\n",
        "        memory.add_to_memory(observation, curvature_action, reward)\n",
        "\n",
        "        # is the episode over? did you crash or do so well that you're done?\n",
        "        if reward == 0.0:\n",
        "            # determine total reward and keep a record of this\n",
        "            total_reward = sum(memory.rewards)\n",
        "            smoothed_reward.append(total_reward)\n",
        "\n",
        "            # execute training step - remember we don't know anything about how the\n",
        "            #   agent is doing until it has crashed! if the training step is too large\n",
        "            #   we need to sample a mini-batch for this step.\n",
        "            batch_size = min(len(memory), max_batch_size)\n",
        "            i = np.random.choice(len(memory), batch_size, replace=False)\n",
        "            train_step(driving_model, compute_driving_loss, optimizer,\n",
        "                               observations=np.array(memory.observations)[i],\n",
        "                               actions=np.array(memory.actions)[i],\n",
        "                               discounted_rewards = discount_rewards(memory.rewards)[i],\n",
        "                               custom_fwd_fn=run_driving_model)\n",
        "            # reset the memory\n",
        "            memory.clear()\n",
        "            break"
      ],
      "metadata": {
        "id": "-eaZuEBRYrSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluation block!##\n",
        "\n",
        "i_step = 0\n",
        "num_episodes = 5\n",
        "num_reset = 5\n",
        "stream = VideoStream()\n",
        "for i_episode in range(num_episodes):\n",
        "\n",
        "    # Restart the environment\n",
        "    vista_reset()\n",
        "    observation = grab_and_preprocess_obs(car)\n",
        "\n",
        "    print(\"rolling out in env\")\n",
        "    episode_step = 0\n",
        "    while not check_crash(car) and episode_step < 100:\n",
        "        # using our observation, choose an action and take it in the environment\n",
        "        curvature_dist = run_driving_model(observation)\n",
        "        curvature = curvature_dist.mean()[0,0]\n",
        "\n",
        "        # Step the simulated car with the same action\n",
        "        vista_step(curvature)\n",
        "        observation = grab_and_preprocess_obs(car)\n",
        "\n",
        "        vis_img = display.render()\n",
        "        stream.write(vis_img[:, :, ::-1], index=i_step)\n",
        "        i_step += 1\n",
        "        episode_step += 1\n",
        "\n",
        "    for _ in range(num_reset):\n",
        "        stream.write(np.zeros_like(vis_img), index=i_step)\n",
        "        i_step += 1\n",
        "\n",
        "print(f\"Average reward: {(i_step - (num_reset*num_episodes)) / num_episodes}\")\n",
        "\n",
        "print(\"Saving trajectory with trained policy...\")\n",
        "stream.save(\"trained_policy.mp4\")\n",
        "mdl.lab3.play_video(\"trained_policy.mp4\")"
      ],
      "metadata": {
        "id": "PLJFXtvRZ4gP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}