{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+HNb40PfKYAYC/4grwsaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avlis-MMO/Deep_Learning/blob/main/MIT_DL/MITLab2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Copyright Information**"
      ],
      "metadata": {
        "id": "OWj5ubWO8aKd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGmfWILp8ENL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Copyright 2023 MIT Introduction to Deep Learning. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of MIT Introduction\n",
        "# to Deep Learning must reference:\n",
        "#\n",
        "# Â© MIT Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 2**\n",
        "# **Part 2: MNIST Digit Classification**\n",
        "# **2.1 Dependencies**"
      ],
      "metadata": {
        "id": "l2NWczgq8uUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Tensorflow 2.0\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import IPython\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Download and import the MIT Introduction to Deep Learning package\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl"
      ],
      "metadata": {
        "id": "vhrCOM618gVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the training data: both images from CelebA and ImageNet\n",
        "path_to_training_data = tf.keras.utils.get_file('train_face.h5', 'https://www.dropbox.com/s/hlz8atheyozp1yx/train_face.h5?dl=1')\n",
        "# Instantiate a TrainingDatasetLoader using the downloaded dataset\n",
        "loader = mdl.lab2.TrainingDatasetLoader(path_to_training_data)\n"
      ],
      "metadata": {
        "id": "oIAibK_E92-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_training_examples = loader.get_train_size()\n",
        "(images, labels) = loader.get_batch(100)"
      ],
      "metadata": {
        "id": "ff9dqLcz-BA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Change the sliders to look at positive and negative training examples! { run: \"auto\" }\n",
        "\n",
        "### Examining the CelebA training dataset ###\n",
        "\n",
        "face_images = images[np.where(labels==1)[0]]\n",
        "not_face_images = images[np.where(labels==0)[0]]\n",
        "\n",
        "idx_face = 12 #@param {type:\"slider\", min:0, max:50, step:1}\n",
        "idx_not_face = 21 #@param {type:\"slider\", min:0, max:50, step:1}\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(face_images[idx_face])\n",
        "plt.title(\"Face\"); plt.grid(False)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(not_face_images[idx_not_face])\n",
        "plt.title(\"Not Face\"); plt.grid(False)"
      ],
      "metadata": {
        "id": "NiaD4iE5-IzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.2 CNN for facial detection**"
      ],
      "metadata": {
        "id": "4-hxz9gW-4ZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Define the CNN model ###\n",
        "\n",
        "n_filters = 24 # base number of convolutional filters\n",
        "\n",
        "'''Function to define a standard CNN model'''\n",
        "def make_standard_classifier(n_outputs=1):\n",
        "  Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n",
        "  BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "  Flatten = tf.keras.layers.Flatten\n",
        "  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "    Conv2D(filters=1*n_filters, kernel_size=5,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(filters=2*n_filters, kernel_size=5,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(filters=4*n_filters, kernel_size=3,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(filters=6*n_filters, kernel_size=3,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512),\n",
        "    Dense(n_outputs, activation=None),\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "standard_classifier = make_standard_classifier()"
      ],
      "metadata": {
        "id": "Rstr0JK2-1f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Train the standard CNN ###\n",
        "\n",
        "# Training hyperparameters\n",
        "batch_size = 32\n",
        "num_epochs = 2  # keep small to run faster\n",
        "learning_rate = 5e-4\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate) # define our optimizer\n",
        "loss_history = mdl.util.LossHistory(smoothing_factor=0.99) # to record loss evolution\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, scale='semilogy')\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "\n",
        "@tf.function\n",
        "def standard_train_step(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # feed the images into the model\n",
        "    logits = standard_classifier(x)\n",
        "    # Compute the loss\n",
        "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "\n",
        "  # Backpropagation\n",
        "  grads = tape.gradient(loss, standard_classifier.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, standard_classifier.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "# The training loop!\n",
        "for epoch in range(num_epochs):\n",
        "  for idx in tqdm(range(loader.get_train_size()//batch_size)):\n",
        "    # Grab a batch of training data and propagate through the network\n",
        "    x, y = loader.get_batch(batch_size)\n",
        "    loss = standard_train_step(x, y)\n",
        "\n",
        "    # Record the loss and plot the evolution of the loss as a function of training\n",
        "    loss_history.append(loss.numpy().mean())\n",
        "    plotter.plot(loss_history.get())"
      ],
      "metadata": {
        "id": "bg7IaosU_vXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluation of standard CNN ###\n",
        "\n",
        "# TRAINING DATA\n",
        "# Evaluate on a subset of CelebA+Imagenet\n",
        "(batch_x, batch_y) = loader.get_batch(5000)\n",
        "y_pred_standard = tf.round(tf.nn.sigmoid(standard_classifier.predict(batch_x)))\n",
        "acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
        "\n",
        "print(\"Standard CNN accuracy on (potentially biased) training set: {:.4f}\".format(acc_standard.numpy()))"
      ],
      "metadata": {
        "id": "0mr5eikLAZR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.3 Variational autoencoder (VAE) for learning latent structure**"
      ],
      "metadata": {
        "id": "mOvb81CfAaDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Defining the VAE loss function ###\n",
        "\n",
        "''' Function to calculate VAE loss given:\n",
        "      an input x,\n",
        "      reconstructed output x_recon,\n",
        "      encoded means mu,\n",
        "      encoded log of standard deviation logsigma,\n",
        "      weight parameter for the latent loss kl_weight\n",
        "'''\n",
        "def vae_loss_function(x, x_recon, mu, logsigma, kl_weight=0.0005):\n",
        "\n",
        "  latent_loss = 1/2 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1.0 - logsigma, axis=1)\n",
        "\n",
        "  reconstruction_loss = tf.reduce_mean(tf.abs(x-x_recon), axis=(1,2,3))\n",
        "\n",
        "  vae_loss = kl_weight * latent_loss + reconstruction_loss\n",
        "\n",
        "  return vae_loss"
      ],
      "metadata": {
        "id": "iCt3hNM0AeGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import zip_longest\n",
        "### VAE Sampling ###\n",
        "\n",
        "\"\"\"Sample latent variables via reparameterization with an isotropic unit Gaussian.\n",
        "# Arguments\n",
        "    z_mean, z_logsigma (tensor): mean and log of standard deviation of latent distribution (Q(z|X))\n",
        "# Returns\n",
        "    z (tensor): sampled latent vector\n",
        "\"\"\"\n",
        "def sampling(z_mean, z_logsigma):\n",
        "  # By default, random.normal is \"standard\" (ie. mean=0 and std=1.0)\n",
        "  batch, latent_dim = z_mean.shape\n",
        "  epsilon = tf.random.normal(shape=(batch, latent_dim))\n",
        "\n",
        "  z = z_mean + tf.math.exp(1/2 * z_logsigma) * epsilon\n",
        "\n",
        "  return z"
      ],
      "metadata": {
        "id": "z3e5W9mMFrN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.5 Semi-supervised variational autoencoder (SS-VAE)**"
      ],
      "metadata": {
        "id": "nIrbG6mGGckz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Loss function for SS-VAE ###\n",
        "\n",
        "\"\"\"Loss function for SS-VAE.\n",
        "# Arguments\n",
        "    x: true input x\n",
        "    x_pred: reconstructed x\n",
        "    y: true label (face or not face)\n",
        "    y_logit: predicted labels\n",
        "    mu: mean of latent distribution (Q(z|X))\n",
        "    logsigma: log of standard deviation of latent distribution (Q(z|X))\n",
        "# Returns\n",
        "    total_loss: SS-VAE total loss\n",
        "    classification_loss: SS-VAE classification loss\n",
        "\"\"\"\n",
        "def ss_vae_loss_function(x, x_pred, y, y_logit, mu, logsigma):\n",
        "\n",
        "  vae_loss = vae_loss_function(x, x_pred, mu, logsigma)\n",
        "\n",
        "  classification_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_logit)\n",
        "\n",
        "  # Use the training data labels to create variable face_indicator:\n",
        "  #   indicator that reflects which training data are images of faces\n",
        "  face_indicator = tf.cast(tf.equal(y, 1), tf.float32)\n",
        "\n",
        "  total_loss = tf.reduce_mean(classification_loss + face_indicator * vae_loss)\n",
        "\n",
        "  return total_loss, classification_loss, vae_loss"
      ],
      "metadata": {
        "id": "orwszEixHAwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Define the decoder portion of the SS-VAE ###\n",
        "\n",
        "def make_face_decoder_network(n_filters=12):\n",
        "\n",
        "  # Functionally define the different layer types we will use\n",
        "  Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu')\n",
        "  BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "  Flatten = tf.keras.layers.Flatten\n",
        "  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
        "  Reshape = tf.keras.layers.Reshape\n",
        "\n",
        "  # Build the decoder network using the Sequential API\n",
        "  decoder = tf.keras.Sequential([\n",
        "    # Transform to pre-convolutional generation\n",
        "    Dense(units=4*4*6*n_filters),  # 4x4 feature maps (with 6N occurances)\n",
        "    Reshape(target_shape=(4, 4, 6*n_filters)),\n",
        "\n",
        "    # Upscaling convolutions (inverse of encoder)\n",
        "    Conv2DTranspose(filters=4*n_filters, kernel_size=3,  strides=2),\n",
        "    Conv2DTranspose(filters=2*n_filters, kernel_size=3,  strides=2),\n",
        "    Conv2DTranspose(filters=1*n_filters, kernel_size=5,  strides=2),\n",
        "    Conv2DTranspose(filters=3, kernel_size=5,  strides=2),\n",
        "  ])\n",
        "\n",
        "  return decoder"
      ],
      "metadata": {
        "id": "qfap-Ra3IOPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Defining and creating the SS-VAE ###\n",
        "\n",
        "class SS_VAE(tf.keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(SS_VAE, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "    # Define the number of outputs for the encoder. Recall that we have\n",
        "    # `latent_dim` latent variables, as well as a supervised output for the\n",
        "    # classification.\n",
        "    num_encoder_dims = 2*self.latent_dim + 1\n",
        "\n",
        "    self.encoder = make_standard_classifier(num_encoder_dims)\n",
        "    self.decoder = make_face_decoder_network()\n",
        "\n",
        "  # function to feed images into encoder, encode the latent space, and output\n",
        "  #   classification probability\n",
        "  def encode(self, x):\n",
        "    # encoder output\n",
        "    encoder_output = self.encoder(x)\n",
        "\n",
        "    # classification prediction\n",
        "    y_logit = tf.expand_dims(encoder_output[:, 0], -1)\n",
        "    # latent variable distribution parameters\n",
        "    z_mean = encoder_output[:, 1:self.latent_dim+1]\n",
        "    z_logsigma = encoder_output[:, self.latent_dim+1:]\n",
        "\n",
        "    return y_logit, z_mean, z_logsigma\n",
        "\n",
        "  # Decode the latent space and output reconstruction\n",
        "  def decode(self, z):\n",
        "\n",
        "    reconstruction = self.decoder(z)\n",
        "    return reconstruction\n",
        "\n",
        "  # The call function will be used to pass inputs x through the core VAE\n",
        "  def call(self, x):\n",
        "    # Encode input to a prediction and latent space\n",
        "    y_logit, z_mean, z_logsigma = self.encode(x)\n",
        "\n",
        "    z = sampling(z_mean, z_logsigma)\n",
        "\n",
        "    recon = self.decode(z)\n",
        "\n",
        "    return y_logit, z_mean, z_logsigma, recon\n",
        "\n",
        "  # Predict face or not face logit for given input x\n",
        "  def predict(self, x):\n",
        "    y_logit, z_mean, z_logsigma = self.encode(x)\n",
        "    return y_logit\n",
        "\n",
        "ss_vae = SS_VAE(latent_dim=32)"
      ],
      "metadata": {
        "id": "pDForhnSIf_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Training the SS-VAE ###\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 5e-4\n",
        "latent_dim = 32\n",
        "\n",
        "# SS-VAE needs slightly more epochs to train since its more complex than\n",
        "# the standard classifier so we use 6 instead of 2\n",
        "num_epochs = 4\n",
        "\n",
        "# instantiate a new SS-VAE model and optimizer\n",
        "ss_vae = SS_VAE(latent_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# To define the training operation, we will use tf.function which is a powerful tool\n",
        "#   that lets us turn a Python function into a TensorFlow computation graph.\n",
        "@tf.function\n",
        "def ss_vae_train_step(x, y):\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Feed input x into ss_vae. Note that this is using the SS_VAE call function!\n",
        "    y_logit, z_mean, z_logsigma, x_recon = ss_vae(x)\n",
        "\n",
        "    loss, class_loss, _ = ss_vae_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma) # TODO\n",
        "\n",
        "  grads = tape.gradient(loss, ss_vae.trainable_variables)\n",
        "\n",
        "  # apply gradients to variables\n",
        "  optimizer.apply_gradients(zip(grads, ss_vae.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "# get training faces from data loader\n",
        "all_faces = loader.get_all_train_faces()\n",
        "\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "\n",
        "# The training loop -- outer loop iterates over the number of epochs\n",
        "for i in range(num_epochs):\n",
        "\n",
        "  IPython.display.clear_output(wait=True)\n",
        "  print(\"Starting epoch {}/{}\".format(i+1, num_epochs))\n",
        "\n",
        "  # get a batch of training data and compute the training step\n",
        "  for j in tqdm(range(loader.get_train_size() // batch_size)):\n",
        "    # load a batch of data\n",
        "    (x, y) = loader.get_batch(batch_size)\n",
        "    # loss optimization\n",
        "    loss = ss_vae_train_step(x, y)\n",
        "\n",
        "    # plot the progress every 200 steps\n",
        "    if j % 500 == 0:\n",
        "      mdl.util.plot_sample(x, y, ss_vae)"
      ],
      "metadata": {
        "id": "O7zJ2iElJmqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.6 Using the SS-VAE to uncover and diagnose biases**"
      ],
      "metadata": {
        "id": "yE5Jim-fGexk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Linking model performance to uncertainty and bias\n",
        "\n",
        "# Load a random sample of 5000 faces from our dataset and compute the model performance on them\n",
        "(x, y) = loader.get_batch(5000, only_faces=True)\n",
        "y_logit, z_mean, z_logsigma, x_recon = ss_vae(x)\n",
        "loss, class_loss, vae_loss = ss_vae_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma)\n",
        "\n",
        "# Sort the results by the vae loss scores\n",
        "vae_loss = vae_loss.numpy()\n",
        "ind = np.argsort(vae_loss, axis=None)\n",
        "\n",
        "# Plot the 25 samples with the highest and lowest reconstruction losses\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
        "ax[0].imshow(mdl.util.create_grid_of_images(x[ind[:25]]))\n",
        "ax[0].set_title(\"Samples with the lowest reconstruction loss \\n\" +\n",
        "                f\"Average recon loss: {np.mean(vae_loss[ind[:25]]):.2f}\")\n",
        "\n",
        "ax[1].imshow(mdl.util.create_grid_of_images(x[ind[-25:]]))\n",
        "ax[1].set_title(\"Samples with the highest reconstruction loss \\n\" +\n",
        "                f\"Average recon loss: {np.mean(vae_loss[ind[-25:]]):.2f}\");"
      ],
      "metadata": {
        "id": "mkJKsHEChpbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Inspect different latent features\n",
        "\n",
        "#@title Change the sliders to inspect different latent features! { run: \"auto\" }\n",
        "idx_latent = 11 #@param {type:\"slider\", min:0, max:31, step:1}\n",
        "num_steps = 15\n",
        "\n",
        "# Extract all latent samples from the desired dimension\n",
        "latent_samples = z_mean[:, idx_latent]\n",
        "\n",
        "# Compute their density and plot\n",
        "density, latent_bins = np.histogram(latent_samples, num_steps, density=True)\n",
        "fig, ax = plt.subplots(2, 1, figsize=(15, 4))\n",
        "ax[0].bar(latent_bins[1:], density)\n",
        "ax[0].set_ylabel(\"Data density\")\n",
        "\n",
        "# Visualize reconstructions as we walk across the latent space\n",
        "latent_steps = np.linspace(np.min(latent_samples), np.max(latent_samples), num_steps)\n",
        "baseline_latent = tf.reduce_mean(z_mean, 0, keepdims=True)\n",
        "\n",
        "recons = []\n",
        "for step in latent_steps:\n",
        "  # Adjust the latent vector according to our step\n",
        "  latent = baseline_latent.numpy()\n",
        "  latent[0, idx_latent] = step\n",
        "  # Decode the reconstruction and store\n",
        "  recons.append(ss_vae.decode(latent)[0])\n",
        "\n",
        "# Visualize all of the reconstructions!\n",
        "ax[1].imshow(mdl.util.create_grid_of_images(recons, (1, num_steps)))\n",
        "ax[1].set_xlabel(\"Latent step\")\n",
        "ax[1].set_ylabel(\"Visualization\");\n"
      ],
      "metadata": {
        "id": "NzQl-x1wLLoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Accuracy vs. density in latent space\n",
        "\n",
        "# Loop through every latent dimension\n",
        "avg_logit_per_bin = []\n",
        "for idx_latent in range(latent_dim):\n",
        "  latent_samples = z_mean[:, idx_latent]\n",
        "  start = np.percentile(latent_samples, 5)\n",
        "  end = np.percentile(latent_samples, 95)\n",
        "  latent_steps = np.linspace(start, end, num_steps)\n",
        "\n",
        "  # Find which samples fall in which bin of the latent dimension\n",
        "  which_latent_bin = np.digitize(latent_samples, latent_steps)\n",
        "\n",
        "  # For each latent bin, compute the accuracy (average logit score)\n",
        "  avg_logit = []\n",
        "  for j in range(0, num_steps+1):\n",
        "    inds_in_bin = np.where(which_latent_bin == j)\n",
        "    avg_logit.append(y_logit.numpy()[inds_in_bin].mean())\n",
        "\n",
        "  avg_logit_per_bin.append(avg_logit)\n",
        "\n",
        "# Average the results across all latent dimensions and all samples\n",
        "accuracy_per_latent = np.mean(avg_logit_per_bin, 0)\n",
        "accuracy_per_latent = (accuracy_per_latent - accuracy_per_latent.min()) / np.ptp(accuracy_per_latent)\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(np.linspace(np.min(z_mean), np.max(z_mean), num_steps+1), accuracy_per_latent,'-o')\n",
        "plt.xlabel(\"Latent step\")\n",
        "plt.ylabel(\"Relative accuracy\")"
      ],
      "metadata": {
        "id": "8UExotVdLP_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}